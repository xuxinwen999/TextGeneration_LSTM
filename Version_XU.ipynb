{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cca0fb75",
   "metadata": {},
   "source": [
    "# Project - Text Generation by LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68500cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a55ee1",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4661f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, build the word dictionary\n",
    "def get_dict(file):\n",
    "    words = ['<bos>','<eos>']\n",
    "    i = 0\n",
    "    with open(file, encoding ='UTF8') as f:\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                words.append(word)\n",
    "\n",
    "    words_dict = list(set(words))\n",
    "    return words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8c66c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sentences\n",
    "def sentences_to_tensor(file):\n",
    "    words_dict = get_dict(file)\n",
    "    sens = []\n",
    "    \n",
    "    with open(file, encoding ='UTF8') as f:\n",
    "        for line in f:\n",
    "            line = \"<bos>\" + line + \"<eos>\"\n",
    "            line = line.split()\n",
    "            sen = np.zeros(len(line))\n",
    "            for index,word in enumerate(line):\n",
    "                sen[index] = words_dict.index(word)\n",
    "                \n",
    "            sens.append(torch.LongTensor(sen))\n",
    "    \n",
    "    return sens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d668cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sentences_to_tensor('./data/ptb.train.txt')\n",
    "dev_data = sentences_to_tensor('./data/ptb.valid.txt')\n",
    "test_data = sentences_to_tensor('./data/ptb.test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f636a07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train dataset: 42068 \n",
      " Number of valid dataset: 3370 \n",
      " Number of test dataset: 3761\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train dataset:\", len(train_data), '\\n', \n",
    "     \"Number of valid dataset:\", len(dev_data), '\\n',\n",
    "     \"Number of test dataset:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb804419",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ebe64f",
   "metadata": {},
   "source": [
    "### Model 1 - Simple LSTMcell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c64ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, inputsize, hiddensize):\n",
    "        super(LSTMCell,self).__init__()\n",
    "        self.inputsize=inputsize\n",
    "        self.hiddensize=hiddensize\n",
    "        \n",
    "        #input\n",
    "        \n",
    "        Wi=torch.zeros(inputsize, hiddensize).cuda()\n",
    "        self.Wi=nn.Parameter(nn.init.xavier_normal_(Wi))\n",
    "        Ui= torch.zeros(hiddensize,hiddensize).cuda()\n",
    "        self.Ui=nn.Parameter(nn.init.xavier_normal_(Ui))\n",
    "        self.bi=nn.Parameter(torch.ones(hiddensize).cuda())\n",
    "        \n",
    "        #forget\n",
    "        \n",
    "        Wf=torch.zeros(inputsize, hiddensize).cuda()\n",
    "        self.Wf=nn.Parameter(nn.init.xavier_normal_(Wf))\n",
    "        Uf= torch.zeros(hiddensize,hiddensize).cuda()\n",
    "        self.Uf=nn.Parameter(nn.init.xavier_normal_(Uf))\n",
    "        self.bf=nn.Parameter(torch.ones(hiddensize).cuda())\n",
    "        \n",
    "        #output\n",
    "        \n",
    "        Wo=torch.zeros(inputsize, hiddensize).cuda()\n",
    "        self.Wo=nn.Parameter(nn.init.xavier_normal_(Wo))\n",
    "        Uo= torch.zeros(hiddensize,hiddensize).cuda()\n",
    "        self.Uo=nn.Parameter(nn.init.xavier_normal_(Uo))\n",
    "        self.bo=nn.Parameter(torch.ones(hiddensize).cuda())\n",
    "        \n",
    "        \n",
    "        #g\n",
    "        \n",
    "        Wg=torch.zeros(inputsize, hiddensize).cuda()\n",
    "        self.Wg=nn.Parameter(nn.init.xavier_normal_(Wg))\n",
    "        Ug= torch.zeros(hiddensize,hiddensize).cuda()\n",
    "        self.Ug=nn.Parameter(nn.init.xavier_normal_(Ug))\n",
    "        self.bg=nn.Parameter(torch.ones(hiddensize).cuda())\n",
    "        \n",
    "    #Implement on sentence level\n",
    "    def forward(self,inp,initialize=None):\n",
    "        seq_len,input_size=inp.size()\n",
    "        hidden_sequence=torch.zeros((seq_len, self.hiddensize)).cuda()\n",
    "        \n",
    "        # Initialize h,c\n",
    "        if initialize is None:\n",
    "            h= torch.zeros(self.hiddensize).cuda()\n",
    "            c= torch.zeros(self.hiddensize).cuda()\n",
    "        else:\n",
    "            h,c=initialize\n",
    "            \n",
    "        #updating gates\n",
    "        for t in range(seq_len-1):    \n",
    "            i=torch.sigmoid(inp[t,:]@self.Wi+h@self.Ui+self.bi)\n",
    "            f=torch.sigmoid(inp[t,:]@self.Wf+h@self.Uf+self.bf)\n",
    "            o=torch.sigmoid(inp[t,:]@self.Wo+h@self.Uo+self.bo)\n",
    "            g=torch.tanh(inp[t,:]@self.Wg+h@self.Ug+self.bg)\n",
    "            c=f*c+i*g\n",
    "            h=o*torch.tanh(c)\n",
    "            \n",
    "            # len(hidden_sequence) = seq_len-1, we don't need (h,c) of <eos> ???\n",
    "            hidden_sequence[t,:] = h\n",
    "            \n",
    "        return hidden_sequence,(h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b6a9ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_LSTM(nn.Module):\n",
    "    def __init__(self,number_words,emb_dim=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding=nn.Embedding(number_words,emb_dim)\n",
    "        self.lstm=LSTMCell(emb_dim,200)\n",
    "        self.output_proj=nn.Linear(200,number_words)\n",
    "            \n",
    "    def forward(self,inp):\n",
    "        input_tensor=self.embedding(inp)\n",
    "        hidden, (h,c)=self.lstm(input_tensor)\n",
    "        output=self.output_proj(hidden)\n",
    "        \n",
    "        return output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31b9ad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, dev_data,epochs=10, lr=0.001, gpu=True):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "        gpu: specify training device\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if gpu:\n",
    "        model.cuda()\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = F.cross_entropy\n",
    "    \n",
    "    #loss list\n",
    "    train_loss = torch.zeros(epochs).cuda()\n",
    "    dev_loss = torch.zeros(epochs).cuda()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(data)\n",
    "        epoch_loss=0\n",
    "        valid_loss=0\n",
    "        model.train()\n",
    "        \n",
    "        for sentence in train_data:\n",
    "            \n",
    "            if gpu:\n",
    "                sentence = sentence.cuda()\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output = model(sentence[:-1])\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            sentence_loss = criterion(output, sentence[1:], reduction=\"sum\")\n",
    "            loss = sentence_loss / len(sentence-1)\n",
    "            loss.backward()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for sentence in dev_data:\n",
    "                sentence = sentence.cuda()\n",
    "                output = model(sentence[:-1])\n",
    "                sentence_loss = criterion(output, sentence[1:], reduction=\"sum\")\n",
    "                loss = sentence_loss / len(sentence-1)\n",
    "                valid_loss += loss.item()\n",
    "                \n",
    "        print(\"Epoch: {}/{} \".format(epoch+1, epochs),\n",
    "                \"avg_train_loss: {:.4f} \".format(epoch_loss/len(train_data)),\n",
    "             \"avg_dev_loss: {:.4f} \".format(valid_loss/len(dev_data)))\n",
    "        \n",
    "        train_loss[epoch] = epoch_loss/len(train_data)\n",
    "        dev_loss[epoch] = valid_loss/len(dev_data)\n",
    "        \n",
    "    return train_loss, dev_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6423b928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple_LSTM(\n",
      "  (embedding): Embedding(10001, 100)\n",
      "  (lstm): LSTMCell()\n",
      "  (output_proj): Linear(in_features=200, out_features=10001, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "word_dict = get_dict('./data/ptb.train.txt')\n",
    "model = Simple_LSTM(len(word_dict))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "befc3428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10  avg_train_loss: 5.1293  avg_dev_loss: 12.1270 \n",
      "Epoch: 2/10  avg_train_loss: 4.6731  avg_dev_loss: 12.7206 \n",
      "Epoch: 3/10  avg_train_loss: 4.5357  avg_dev_loss: 13.0890 \n",
      "Epoch: 4/10  avg_train_loss: 4.4533  avg_dev_loss: 13.4295 \n",
      "Epoch: 5/10  avg_train_loss: 4.3917  avg_dev_loss: 13.5033 \n",
      "Epoch: 6/10  avg_train_loss: 4.3484  avg_dev_loss: 13.8617 \n",
      "Epoch: 7/10  avg_train_loss: 4.3212  avg_dev_loss: 14.1961 \n",
      "Epoch: 8/10  avg_train_loss: 4.2995  avg_dev_loss: 14.7355 \n",
      "Epoch: 9/10  avg_train_loss: 4.2812  avg_dev_loss: 15.4136 \n",
      "Epoch: 10/10  avg_train_loss: 4.2687  avg_dev_loss: 15.4750 \n"
     ]
    }
   ],
   "source": [
    "list1, list2 = train(model, train_data, dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7d8a8b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generation(num, file, data, dictionary, model):\n",
    "    ''' Word predition\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        num: number of sentences to generate\n",
    "        file: \n",
    "    '''\n",
    "    \n",
    "    with open(file, encoding ='UTF8') as f:\n",
    "            for i in range(num):\n",
    "                line = f.readline()\n",
    "                \n",
    "                logits = model(data[i].cuda())\n",
    "                pred = logits.argmax(dim=1).cpu()\n",
    "                sentence = ''\n",
    "            \n",
    "                for index in pred:\n",
    "                    word = dictionary[index]\n",
    "                    sentence = sentence + word + ' '\n",
    "                \n",
    "                print(\"Sentence {}:\".format(i), '\\t', line,\n",
    "                     \"Prediction:\", '\\t', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c0c894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
