{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258516de",
   "metadata": {},
   "source": [
    "# Project - Gating Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7143ad2",
   "metadata": {},
   "source": [
    "##### Lili LU <br> Xiqing Chang <br> Xinwen XU <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dc9921c",
   "metadata": {
    "id": "0dc9921c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import init\n",
    "from torch import Tensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c8917d",
   "metadata": {
    "id": "d0c8917d"
   },
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a504d95",
   "metadata": {
    "id": "1a504d95"
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271f72f",
   "metadata": {
    "id": "9271f72f"
   },
   "outputs": [],
   "source": [
    "# read the file\n",
    "def read_file(path):\n",
    "    data = list()\n",
    "    with open(path) as inf:\n",
    "        for line in inf:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            data.append(line.split())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acb3ec2",
   "metadata": {
    "id": "9acb3ec2"
   },
   "outputs": [],
   "source": [
    "# read the train data, dev data and test data\n",
    "train_data = read_file(\"./ptb.train.txt\")\n",
    "dev_data = read_file(\"./ptb.valid.txt\")\n",
    "test_data = read_file(\"./ptb.test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3893256",
   "metadata": {
    "id": "d3893256"
   },
   "outputs": [],
   "source": [
    "# create a set of all the words in the train data\n",
    "train_words = set()\n",
    "for sentence in train_data:\n",
    "    train_words.update(sentence)\n",
    "\n",
    "train_words.update([\"<bos>\", \"<eos>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2JtlEqRVJekL",
   "metadata": {
    "id": "2JtlEqRVJekL"
   },
   "source": [
    "## Illustration of data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dBjnFMGYGG",
   "metadata": {
    "id": "c5dBjnFMGYGG"
   },
   "outputs": [],
   "source": [
    "# training set\n",
    "len_dist = [0] * 5\n",
    "for sentence in train_data:\n",
    "  if len(sentence) <= 10: len_dist[0] += 1\n",
    "  elif len(sentence) > 10 and len(sentence) <= 20: len_dist[1] += 1\n",
    "  elif len(sentence) > 20 and len(sentence) <= 30: len_dist[2] += 1\n",
    "  elif len(sentence) > 30 and len(sentence) <= 40: len_dist[3] += 1\n",
    "  else: len_dist[4] += 1\n",
    "\n",
    "\n",
    "plt.xlabel('length of sentence')\n",
    "plt.ylabel('number of sentence')\n",
    "\n",
    "\n",
    "bar_labels = [ \"[0, 10]\", \"[10, 20]\", \"[20, 30]\", \"[30, 40]\", \"[40, :]\"]\n",
    "y_pos = np.arange(len(len_dist))\n",
    "\n",
    "bars = plt.bar(y_pos, len_dist)\n",
    "plt.xticks(y_pos, bar_labels)\n",
    "for b, d in zip(bars, len_dist):\n",
    "  plt.text(b.get_x() + 0.15, b.get_height() + 100,'{0:.2%}'.format(d/sum(len_dist)))\n",
    "\n",
    "plt.xlabel('length of sentence')\n",
    "plt.ylabel('number of sentence')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FRRzG48dJmY0",
   "metadata": {
    "id": "FRRzG48dJmY0"
   },
   "outputs": [],
   "source": [
    "# valid set\n",
    "len_dist = [0] * 5\n",
    "for sentence in dev_data:\n",
    "  if len(sentence) <= 10: len_dist[0] += 1\n",
    "  elif len(sentence) > 10 and len(sentence) <= 20: len_dist[1] += 1\n",
    "  elif len(sentence) > 20 and len(sentence) <= 30: len_dist[2] += 1\n",
    "  elif len(sentence) > 30 and len(sentence) <= 40: len_dist[3] += 1\n",
    "  else: len_dist[4] += 1\n",
    "\n",
    "bar_labels = [ \"[0, 10]\", \"[10, 20]\", \"[20, 30]\", \"[30, 40]\", \"[40, :]\"]\n",
    "y_pos = np.arange(len(len_dist))\n",
    "\n",
    "bars = plt.bar(y_pos, len_dist)\n",
    "plt.xticks(y_pos, bar_labels)\n",
    "for b, d in zip(bars, len_dist):\n",
    "  plt.text(b.get_x() + 0.15, b.get_height()+ 10,'{0:.2%}'.format(d/sum(len_dist)))\n",
    "\n",
    "plt.xlabel('length of sentence')\n",
    "plt.ylabel('number of sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T8dV5QPRJmB6",
   "metadata": {
    "id": "T8dV5QPRJmB6"
   },
   "outputs": [],
   "source": [
    "# test set\n",
    "len_dist = [0] * 5\n",
    "for sentence in test_data:\n",
    "  if len(sentence) <= 10: len_dist[0] += 1\n",
    "  elif len(sentence) > 10 and len(sentence) <= 20: len_dist[1] += 1\n",
    "  elif len(sentence) > 20 and len(sentence) <= 30: len_dist[2] += 1\n",
    "  elif len(sentence) > 30 and len(sentence) <= 40: len_dist[3] += 1\n",
    "  else: len_dist[4] += 1\n",
    "\n",
    "bar_labels = [ \"[0, 10]\", \"[10, 20]\", \"[20, 30]\", \"[30, 40]\", \"[40, :]\"]\n",
    "y_pos = np.arange(len(len_dist))\n",
    "\n",
    "bars = plt.bar(y_pos, len_dist)\n",
    "plt.xticks(y_pos, bar_labels)\n",
    "for b, d in zip(bars, len_dist):\n",
    "  plt.text(b.get_x() + 0.15, b.get_height()+ 10,'{0:.2%}'.format(d/sum(len_dist)))\n",
    "\n",
    "plt.xlabel('length of sentence')\n",
    "plt.ylabel('number of sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e86445",
   "metadata": {
    "id": "81e86445"
   },
   "source": [
    "## Create word dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7431d0",
   "metadata": {
    "id": "3f7431d0"
   },
   "outputs": [],
   "source": [
    "class create_word_dic():\n",
    "    def __init__(self, words):\n",
    "        self.dict_word_id = dict()\n",
    "        self.dict_id_word = list()\n",
    "        \n",
    "        for idx, word in enumerate(words):\n",
    "            self.dict_word_id[word] = idx\n",
    "            self.dict_id_word.append(word)\n",
    "    \n",
    "    def word_to_id(self, word):\n",
    "        return self.dict_word_id[word]\n",
    "    \n",
    "    def id_to_word(self, idx):\n",
    "        return self.dict_id_word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc25fe",
   "metadata": {
    "id": "84cc25fe"
   },
   "outputs": [],
   "source": [
    "word_dict = create_word_dic(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415774b7",
   "metadata": {
    "id": "415774b7"
   },
   "outputs": [],
   "source": [
    "print(word_dict.word_to_id('<eos>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3411c1a2",
   "metadata": {
    "id": "3411c1a2"
   },
   "outputs": [],
   "source": [
    "print(word_dict.id_to_word(7492))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6cd719",
   "metadata": {
    "id": "5f6cd719"
   },
   "source": [
    "## transfer to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe81e21",
   "metadata": {
    "id": "0fe81e21"
   },
   "outputs": [],
   "source": [
    "# transfer the data into tensor type\n",
    "def sentence_to_tensor(word_dict, sentence):\n",
    "    sentence = [word_dict.word_to_id(word) for word in sentence]\n",
    "    bos = word_dict.word_to_id('<bos>')\n",
    "    eos = word_dict.word_to_id('<eos>')\n",
    "    return torch.tensor([bos] + sentence + [eos])\n",
    "\n",
    "def data_to_tensor(word_dict, data):\n",
    "    return [sentence_to_tensor(word_dict, sentence) for sentence in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623fd61f",
   "metadata": {
    "id": "623fd61f"
   },
   "outputs": [],
   "source": [
    "train_tensor = data_to_tensor(word_dict, train_data)\n",
    "valid_tensor = data_to_tensor(word_dict, dev_data)\n",
    "test_tensor = data_to_tensor(word_dict, test_data)\n",
    "print(len(train_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BOU_WKo6jTzr",
   "metadata": {
    "id": "BOU_WKo6jTzr"
   },
   "source": [
    "# This experiment executes the training and evaluation with batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579792f9",
   "metadata": {
    "id": "579792f9"
   },
   "source": [
    "# Naive LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72085a4b",
   "metadata": {
    "id": "72085a4b"
   },
   "source": [
    "## Naive LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa19082c",
   "metadata": {
    "id": "aa19082c"
   },
   "outputs": [],
   "source": [
    "class Naive_LSTM_cell(nn.Module):\n",
    "    \"\"\"Naive LSTM like nn.LSTM\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Naive_LSTM_cell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # input gate\n",
    "        self.W_i = Parameter(Tensor(hidden_size, input_size))\n",
    "        self.U_i = Parameter(Tensor(hidden_size, hidden_size))\n",
    "        self.b_i = Parameter(Tensor(hidden_size, 1))\n",
    "        \n",
    "        # forget gate\n",
    "        self.W_f = Parameter(Tensor(hidden_size, input_size))\n",
    "        self.U_f = Parameter(Tensor(hidden_size, hidden_size))\n",
    "        self.b_f = Parameter(Tensor(hidden_size, 1))\n",
    "        \n",
    "        # output gate\n",
    "        self.W_o = Parameter(Tensor(hidden_size, input_size))\n",
    "        self.U_o = Parameter(Tensor(hidden_size, hidden_size))\n",
    "        self.b_o = Parameter(Tensor(hidden_size, 1))\n",
    "        \n",
    "        # cell\n",
    "        self.W_g = Parameter(Tensor(hidden_size, input_size))\n",
    "        self.U_g = Parameter(Tensor(hidden_size, hidden_size))\n",
    "        self.b_g = Parameter(Tensor(hidden_size, 1))\n",
    "        \n",
    "        # initialize the weights\n",
    "        self.initialize_weights()\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        \"\"\"initialize weights\n",
    "        \"\"\"\n",
    "        '''\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            init.uniform_(weight, -stdv, stdv)\n",
    "        '''\n",
    "        for weight in self.parameters():\n",
    "            init.xavier_normal_(weight)\n",
    "    \n",
    "    def forward(self, inputs, state = None):\n",
    "        \"\"\"Forward\n",
    "        Args:\n",
    "            inputs: [1, 1, input_size]\n",
    "            state: ([1, 1, hidden_size], [1, 1, hidden_size])\n",
    "        \"\"\"\n",
    "        batch_size, seq_size, _ = inputs.size()\n",
    "        hidden_seq = []\n",
    "        \n",
    "        # read the state\n",
    "        if state is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size).t().to(inputs.device)\n",
    "            c_t = torch.zeros(batch_size, self.hidden_size).t().to(inputs.device)\n",
    "        else:\n",
    "            (h, c) = state\n",
    "            h_t = h.squeeze(0).t()\n",
    "            c_t = c.squeeze(0).t()\n",
    "        \n",
    "        # for each sequence, do the iteration\n",
    "        for t in range(seq_size):\n",
    "            x_t = inputs[:, t, :].t()\n",
    "            \n",
    "            # input gate\n",
    "            i_t = torch.sigmoid(self.W_i @ x_t + self.U_i @ h_t + self.b_i)\n",
    "            # forget gate\n",
    "            f_t = torch.sigmoid(self.W_f @ x_t + self.U_f @ h_t + self.b_f)\n",
    "            # cell\n",
    "            g_t = torch.tanh(self.W_g @ x_t + self.U_g @ h_t + self.b_g)\n",
    "            # output gate\n",
    "            o_t = torch.sigmoid(self.W_o @ x_t + self.U_o @ h_t + self.b_o)\n",
    "            \n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            \n",
    "            hidden_seq.append(h_t.t().unsqueeze(0))\n",
    "        \n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0,1).contiguous()\n",
    "        \n",
    "        return hidden_seq, (h_t.t().unsqueeze(0), c_t.t().unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ed889d",
   "metadata": {
    "id": "e4ed889d"
   },
   "source": [
    "## Naive LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3311e61c",
   "metadata": {
    "id": "3311e61c"
   },
   "outputs": [],
   "source": [
    "# without batch\n",
    "class Naive_LSTM(nn.Module):\n",
    "    def __init__(self, total_num_words, emb_size=100, lstm_hidden_size=200, n_lstm_layers=1):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(total_num_words, emb_size)\n",
    "        self.lstm = Naive_LSTM_cell(input_size = emb_size, hidden_size = lstm_hidden_size)\n",
    "        self.output_proj = nn.Linear(lstm_hidden_size, total_num_words)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "            \n",
    "        embedding = self.embeddings(inp)\n",
    "        hidden, _ = self.lstm(embedding.unsqueeze(0))\n",
    "        output_logics = self.output_proj(hidden)\n",
    "        \n",
    "        return output_logics.squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e196ad",
   "metadata": {
    "id": "89e196ad"
   },
   "outputs": [],
   "source": [
    "\n",
    "# with batch\n",
    "class Naive_LSTM_batch(nn.Module):\n",
    "    def __init__(self, total_num_words, emb_size=100, lstm_hidden_size=200, n_lstm_layers=1):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(total_num_words+1, emb_size, padding_idx = len(train_words))\n",
    "        self.lstm = Naive_LSTM_cell(input_size = emb_size, hidden_size = lstm_hidden_size)\n",
    "        self.output_proj = nn.Linear(lstm_hidden_size, total_num_words)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        embedding = self.embeddings(input_tensor)\n",
    "        hidden, (h, c) = self.lstm(embedding)\n",
    "        output_logics = self.output_proj(hidden)\n",
    "        \n",
    "        return output_logics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63060aa",
   "metadata": {
    "id": "c63060aa"
   },
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd72316",
   "metadata": {
    "id": "0dd72316"
   },
   "source": [
    "### without batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cb92b3",
   "metadata": {
    "id": "23cb92b3"
   },
   "outputs": [],
   "source": [
    "# naive lstm training model without batch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "naive_lstm = Naive_LSTM(len(train_words)).to(device)\n",
    "\n",
    "# optimizer \n",
    "optimizer = torch.optim.Adam(naive_lstm.parameters(), lr = 1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma=0.9)\n",
    "\n",
    "# training\n",
    "all_train_epoch_losses = []\n",
    "all_valid_epoch_losses = []\n",
    "all_epoch_perplexity = []\n",
    "\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    # learning\n",
    "    random.shuffle(train_tensor)\n",
    "    train_epoch_loss = 0\n",
    "    valid_epoch_loss = 0\n",
    "    naive_lstm.train()\n",
    "\n",
    "    for sentence in train_tensor:\n",
    "        optimizer.zero_grad() ## zero the parameters of gradients\n",
    "        logits = naive_lstm(sentence[:-1].to(device))\n",
    "        sentence_loss = F.cross_entropy(logits, sentence[1:].to(device), reduction=\"sum\")\n",
    "\n",
    "        train_epoch_loss += sentence_loss.item()\n",
    "        loss = sentence_loss / (len(sentence)-1)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "        \n",
    "    # evaluation   \n",
    "    naive_lstm.eval()\n",
    "    n_valid_perplexity = []\n",
    "                     \n",
    "    with torch.no_grad():\n",
    "        for sentence in valid_tensor:\n",
    "            logits = naive_lstm(sentence[:-1].to(device))\n",
    "            # calculate the loss\n",
    "            sentence_loss = F.cross_entropy(logits, sentence[1:].to(device), reduction=\"sum\")\n",
    "            valid_epoch_loss += sentence_loss.item()\n",
    "\n",
    "            # calculate the perplexity\n",
    "            n_valid_perplexity.append(torch.exp(F.cross_entropy(logits, sentence[1:].to(device),reduction='mean')).item())\n",
    "                     \n",
    "    all_train_epoch_losses.append(train_epoch_loss / len(train_tensor))\n",
    "    all_valid_epoch_losses.append(valid_epoch_loss / len(valid_tensor))\n",
    "    all_epoch_perplexity.append(np.sum(n_valid_perplexity) / len(n_valid_perplexity)) \n",
    "    \n",
    "    print(\n",
    "        epoch, \":\\t / loss for train data:\\t\",\n",
    "        train_epoch_loss / len(train_tensor),\n",
    "        \"\\t / loss for validation data:\\t\",\n",
    "        valid_epoch_loss / len(valid_tensor),\n",
    "        \"\\t / perplexity:\\t\",\n",
    "        np.sum(n_valid_perplexity) / len(n_valid_perplexity),\n",
    "        flush=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbada22",
   "metadata": {
    "id": "efbada22"
   },
   "source": [
    "### with batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af9220",
   "metadata": {
    "id": "b8af9220"
   },
   "outputs": [],
   "source": [
    "# naive lstm training model with batch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "naive_lstm_batch = Naive_LSTM_batch(len(train_words)).to(device)\n",
    "\n",
    "# optimizer \n",
    "optimizer = torch.optim.Adam(naive_lstm_batch.parameters(), lr = 2e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma=0.9)\n",
    "\n",
    "# epochs and batchsize\n",
    "n_epochs = 20\n",
    "batch_size = 10\n",
    "\n",
    "# training\n",
    "all_train_epoch_losses = []\n",
    "all_valid_epoch_losses = []\n",
    "all_epoch_perplexity = []\n",
    "all_epoch_accuracies = []\n",
    "\n",
    "# training\n",
    "for epoch in range(n_epochs):\n",
    "    random.shuffle(train_tensor)\n",
    "    train_epoch_loss = 0\n",
    "    valid_epoch_loss = 0\n",
    "    naive_lstm_batch.train()\n",
    "\n",
    "    for batch in range(0, len(train_tensor), batch_size):\n",
    "        batch_data = train_tensor[batch:batch+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # find the sentence with the maximum length\n",
    "        sentence_length = [len(sentence) for sentence in batch_data]\n",
    "        max_len = max(sentence_length)\n",
    "\n",
    "        # pack up all the sentences of one batch into a matrix\n",
    "        batched_input = torch.empty(len(batch_data), max_len-1, dtype=torch.int64).to(device)\n",
    "        batched_mask = torch.zeros(len(batch_data), max_len-1, dtype=torch.bool).to(device)\n",
    "        batch_labels = torch.zeros(len(batch_data), max_len-1, dtype=torch.int64).to(device)\n",
    "        \n",
    "        # initialize the matrix\n",
    "        batched_input.fill_(naive_lstm_batch.embeddings.padding_idx)\n",
    "\n",
    "        # update the matrix \n",
    "        for i in range(len(batch_data)):\n",
    "            sentence = batch_data[i]\n",
    "            batched_input[i][:len(sentence) - 1] = sentence[:-1]\n",
    "            batched_mask[i][:len(sentence) - 1] = True\n",
    "            batch_labels[i][:len(sentence) - 1] = sentence[1:]\n",
    "\n",
    "        # calculate the output logits\n",
    "        batch_logits = naive_lstm_batch(batched_input)\n",
    "        batch_logits = batch_logits[batched_mask]\n",
    "        batch_labels = batch_labels[batched_mask]\n",
    "        \n",
    "        # calculate the cross entropy as the training loss function\n",
    "        batch_loss = F.cross_entropy(batch_logits, batch_labels, reduction=\"sum\")\n",
    "        batch_norm = sum(len(s) - 1 for s in batch_data)\n",
    "            \n",
    "        train_epoch_loss += batch_loss.item()\n",
    "        loss = batch_loss / batch_norm\n",
    "        # backwards\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # evaluation   \n",
    "    naive_lstm_batch.eval()\n",
    "    n_valid_perplexity = []\n",
    "    accuracy = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in range(0, len(valid_tensor), batch_size):\n",
    "            batch_data = valid_tensor[batch:batch+batch_size]\n",
    "\n",
    "            # find the sentence with the maximum length\n",
    "            sentence_length = [len(sentence) for sentence in batch_data]\n",
    "            max_len = max(sentence_length)\n",
    "            \n",
    "            # pack up all the sentences of one batch into a matrix\n",
    "            batched_input = torch.empty(len(batch_data), max_len-1, dtype=torch.int64).to(device)\n",
    "            batched_mask = torch.zeros(len(batch_data), max_len-1, dtype=torch.bool).to(device)\n",
    "            batch_labels = torch.zeros(len(batch_data), max_len-1, dtype=torch.int64).to(device)\n",
    "            \n",
    "            # initialize the matrix\n",
    "            batched_input.fill_(naive_lstm_batch.embeddings.padding_idx)\n",
    "            \n",
    "            # update the matrix \n",
    "            for i in range(len(batch_data)):\n",
    "                sentence = batch_data[i]\n",
    "                batched_input[i][:len(sentence) - 1] = sentence[:-1]\n",
    "                batched_mask[i][:len(sentence) - 1] = True\n",
    "                batch_labels[i][:len(sentence) - 1] = sentence[1:]\n",
    "            \n",
    "            # calculate the output logits\n",
    "            batch_logits = naive_lstm_batch(batched_input)\n",
    "            batch_logits = batch_logits[batched_mask]\n",
    "            batch_labels = batch_labels[batched_mask]\n",
    "\n",
    "            # calculate the cross entropy as the loss function\n",
    "            batch_loss = F.cross_entropy(batch_logits, batch_labels, reduction=\"sum\")\n",
    "            batch_norm = sum(len(s) - 1 for s in batch_data)\n",
    "            valid_epoch_loss += batch_loss.item()\n",
    "\n",
    "            # calculate the perplexity\n",
    "            n_valid_perplexity.append(torch.exp(F.cross_entropy(batch_logits, batch_labels, reduction=\"mean\")).item())\n",
    "            \n",
    "            # calculate the accuracy\n",
    "            accuracy.append((batch_labels == batch_logits.argmax(dim=1)).sum().item() / len(batch_labels))\n",
    "\n",
    "    all_train_epoch_losses.append(train_epoch_loss / len(train_tensor))\n",
    "    all_valid_epoch_losses.append(valid_epoch_loss / len(valid_tensor))\n",
    "    all_epoch_perplexity.append(np.sum(n_valid_perplexity)/(len(n_valid_perplexity)))\n",
    "    all_epoch_accuracies.append(np.sum(accuracy) / len(accuracy))\n",
    "\n",
    "    print(\n",
    "        epoch, \":\\t / loss for train data:\\t\",\n",
    "        train_epoch_loss / len(train_tensor),\n",
    "        \"\\t / loss for validation data:\\t\",\n",
    "        valid_epoch_loss / len(valid_tensor),\n",
    "        \"\\t / perplexity:\\t\",\n",
    "        np.sum(n_valid_perplexity) / (len(n_valid_perplexity)),\n",
    "        \"\\t / accuracy:\\t\",\n",
    "        np.sum(accuracy) / len(accuracy),\n",
    "        flush = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Nqg1CNNrkKLy",
   "metadata": {
    "id": "Nqg1CNNrkKLy"
   },
   "outputs": [],
   "source": [
    "# save data\n",
    "naive_perplexity = torch.tensor(all_epoch_perplexity)\n",
    "naive_accuracies = torch.tensor(all_epoch_accuracies)\n",
    "naive_train_losses = torch.tensor(all_train_epoch_losses)\n",
    "naive_valid_losses = torch.tensor(all_valid_epoch_losses)\n",
    "\n",
    "torch.save(naive_perplexity, \"./naive_perplexity.pt\")\n",
    "torch.save(naive_accuracies, \"./naive_accuracies.pt\")\n",
    "torch.save(naive_train_losses, \"./naive_train_losses.pt\")\n",
    "torch.save(naive_valid_losses, \"./naive_valid_losses.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49692df",
   "metadata": {
    "id": "d49692df"
   },
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3alV07TF2rz",
   "metadata": {
    "id": "e3alV07TF2rz"
   },
   "outputs": [],
   "source": [
    "x1 = range(0, n_epochs)\n",
    "x2 = range(0, n_epochs)\n",
    "x3 = range(0, n_epochs)\n",
    "x4 = range(0, n_epochs)\n",
    "\n",
    "y1 = all_epoch_perplexity\n",
    "y2 = all_train_epoch_losses\n",
    "y3 = all_valid_epoch_losses\n",
    "y4 = all_epoch_accuracies\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(x1, y1)\n",
    "plt.title('Valid perplexity vs. epoches')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Valid perplexity')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(x4, y4)\n",
    "plt.title('Accuracy vs. epoches')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x2, y2,  label='Train loss')\n",
    "plt.plot(x3, y3, label='Valid loss')\n",
    "plt.title('Train/Valid loss vs. epoches')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20275ff",
   "metadata": {
    "id": "e20275ff"
   },
   "source": [
    "# Peephole LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7ef29b",
   "metadata": {
    "id": "7f7ef29b"
   },
   "source": [
    "## Peephole LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64431aa2",
   "metadata": {
    "id": "64431aa2"
   },
   "outputs": [],
   "source": [
    "class Peephole_LSTM_cell(nn.Module):\n",
    "    \"\"\"Peephole LSTM \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Peephole_LSTM_cell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # input gate\n",
    "        self.W_i = Parameter(Tensor(hidden_size, input_size))\n",
    "        self.U_i = Parameter(Tensor(hidden_size, hidden_size))\n",
    "        self.P_i = Parameter(Tensor(1, hidden_size))\n",
    "        self.b_i = Parameter(Tensor(hidden_size, 1))\n",
    "        \n",
    "        # forget gate\n",
    "        self.W_f = Parameter(Tensor(hidden_size, input_size))\n",
    "        self.U_f = Parameter(Tensor(hidden_size, hidden_size))\n",
    "        self.P_f = Parameter(Tensor(1, hidden_size))\n",
    "        self.b_f = Parameter(Tensor(hidden_size, 1))\n",
    "        \n",
    "        # output gate\n",
    "        self.W_o = Parameter(Tensor(hidden_size, input_size))\n",
    "        self.U_o = Parameter(Tensor(hidden_size, hidden_size))\n",
    "        self.P_o = Parameter(Tensor(1, hidden_size))\n",
    "        self.b_o = Parameter(Tensor(hidden_size, 1))\n",
    "        \n",
    "        # cell\n",
    "        self.W_g = Parameter(Tensor(hidden_size, input_size))\n",
    "        self.U_g = Parameter(Tensor(hidden_size, hidden_size))\n",
    "        self.b_g = Parameter(Tensor(hidden_size, 1))\n",
    "        \n",
    "        # initialize the weights\n",
    "        self.initialize_weights()\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        \"\"\"initialize weights\n",
    "        \"\"\"\n",
    "        '''\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            init.uniform_(weight, -stdv, stdv)\n",
    "        '''\n",
    "        for weight in self.parameters():\n",
    "            init.xavier_normal_(weight)\n",
    "        \n",
    "    def forward(self, inputs, state = None):\n",
    "        \"\"\"Forward\n",
    "        Args:\n",
    "            inputs: [1, 1, input_size]\n",
    "            state: ([1, 1, hidden_size], [1, 1, hidden_size])\n",
    "        \"\"\"\n",
    "        batch_size, seq_size, _ = inputs.size()\n",
    "        hidden_seq = []\n",
    "        \n",
    "        # read the state\n",
    "        if state is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size).t().to(inputs.device)\n",
    "            c_t = torch.zeros(batch_size, self.hidden_size).t().to(inputs.device)\n",
    "        else:\n",
    "            (h, c) = state\n",
    "            h_t = h.squeeze(0).t()\n",
    "            c_t = c.squeeze(0).t()\n",
    " \n",
    "        # for each sequence, do the iteration\n",
    "        for t in range(seq_size):\n",
    "            x_t = inputs[:, t, :].t()\n",
    "            \n",
    "            # input gate\n",
    "            i_t = torch.sigmoid(self.W_i @ x_t + self.U_i @ h_t + self.P_i @ c_t + self.b_i) \n",
    "            # forget gate\n",
    "            f_t = torch.sigmoid(self.W_f @ x_t + self.U_f @ h_t + self.P_f @ c_t + self.b_f)\n",
    "            # cell\n",
    "            g_t = torch.tanh(self.W_g @ x_t + self.U_g @ h_t + self.b_g)\n",
    "            # output gate\n",
    "            o_t = torch.sigmoid(self.W_o @ x_t + self.U_o @ h_t + self.P_o @ c_t + self.b_o)\n",
    "            \n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "           \n",
    "            hidden_seq.append(h_t.t().unsqueeze(0))\n",
    "        \n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq =hidden_seq.transpose(0,1).contiguous()\n",
    "        \n",
    "        return hidden_seq, (h_t.t().unsqueeze(0), c_t.t().unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63ff3ea",
   "metadata": {
    "id": "d63ff3ea"
   },
   "source": [
    "## Peephole LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c55c95",
   "metadata": {
    "id": "72c55c95"
   },
   "outputs": [],
   "source": [
    "# without batch\n",
    "class Peephole_LSTM(nn.Module):\n",
    "    def __init__(self, total_num_words, emb_size=100, lstm_hidden_size=200, n_lstm_layers=1):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(total_num_words, emb_size)\n",
    "        self.lstm = Peephole_LSTM_cell(input_size = emb_size, hidden_size = lstm_hidden_size)\n",
    "        self.output_proj = nn.Linear(lstm_hidden_size, total_num_words)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        embedding = self.embeddings(input_tensor)\n",
    "        hidden, (h, c) = self.lstm(embedding.unsqueeze(0))\n",
    "        output_logics = self.output_proj(hidden)\n",
    "        \n",
    "        return output_logics.squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f5ba51",
   "metadata": {
    "id": "00f5ba51"
   },
   "outputs": [],
   "source": [
    "# with batch\n",
    "class Peephole_LSTM_batch(nn.Module):\n",
    "    def __init__(self, total_num_words, emb_size=100, lstm_hidden_size=200, n_lstm_layers=1):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(total_num_words+1, emb_size, padding_idx = len(train_words))\n",
    "        self.lstm = Peephole_LSTM_cell(input_size = emb_size, hidden_size = lstm_hidden_size)\n",
    "        self.output_proj = nn.Linear(lstm_hidden_size, total_num_words)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        embedding = self.embeddings(input_tensor)\n",
    "        hidden, (h, c) = self.lstm(embedding)\n",
    "        output_logics = self.output_proj(hidden)\n",
    "        \n",
    "        return output_logics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf2c783",
   "metadata": {
    "id": "aaf2c783"
   },
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d021a90",
   "metadata": {
    "id": "0d021a90"
   },
   "source": [
    "### without batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62b41e",
   "metadata": {
    "id": "ca62b41e"
   },
   "outputs": [],
   "source": [
    "# peephole lstm training model without batch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "peephole_lstm = Peephole_LSTM(len(train_words)).to(device)\n",
    "\n",
    "# optimizer \n",
    "optimizer = torch.optim.Adam(peephole_lstm.parameters(), lr = 1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma=0.9)\n",
    "\n",
    "# training\n",
    "all_epoch_train_losses = []\n",
    "all_epoch_valid_losses = []\n",
    "all_epoch_perplexity = []\n",
    "\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    # learning\n",
    "    random.shuffle(train_tensor)\n",
    "    train_epoch_loss = 0\n",
    "    valid_epoch_loss = 0\n",
    "    peephole_lstm.train()\n",
    "    for sentence in train_tensor:\n",
    "        optimizer.zero_grad() ## zero the parameters of gradients\n",
    "        logits = peephole_lstm(sentence[:-1].to(device)) \n",
    "        sentence_loss = F.cross_entropy(logits, sentence[1:].to(device), reduction=\"sum\")\n",
    "        \n",
    "        train_epoch_loss += sentence_loss.item()\n",
    "        loss = sentence_loss / (len(sentence)-1)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "        \n",
    "    # evaluation   \n",
    "    peephole_lstm.eval()\n",
    "    n_valid_perplexity = []\n",
    "                     \n",
    "    with torch.no_grad():\n",
    "        for sentence in valid_tensor:\n",
    "            logits = peephole_lstm(sentence[:-1].to(device))\n",
    "            sentence_loss = F.cross_entropy(logits, sentence[1:].to(device), reduction=\"sum\")\n",
    "            valid_epoch_loss += sentence_loss.item()\n",
    "              \n",
    "            n_valid_perplexity.append(torch.exp(F.cross_entropy(logits, sentence[1:].to(device),reduction='mean')).item()) \n",
    "            \n",
    "            \n",
    "    all_train_epoch_losses.append(train_epoch_loss / len(train_tensor))\n",
    "    all_valid_epoch_losses.append(valid_epoch_loss / len(valid_tensor))\n",
    "    all_epoch_perplexity.append(np.sum(n_valid_perplexity) / len(n_valid_perplexity)) \n",
    "    \n",
    "    print(\n",
    "        epoch, \":\\t / loss for train data:\\t\",\n",
    "        train_epoch_loss / len(train_tensor),\n",
    "        \"\\t / loss for validation data:\\t\",\n",
    "        valid_epoch_loss / len(valid_tensor),\n",
    "        \"\\t / perplexity:\\t\",\n",
    "        np.sum(n_valid_perplexity) / len(n_valid_perplexity),\n",
    "        flush=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2465c6",
   "metadata": {
    "id": "cf2465c6"
   },
   "source": [
    "### with batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7c8f18",
   "metadata": {
    "id": "fb7c8f18"
   },
   "outputs": [],
   "source": [
    "# peephole lstm model with batch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "peephole_lstm_batch = Peephole_LSTM_batch(len(train_words)).to(device)\n",
    "\n",
    "# optimizer \n",
    "optimizer = torch.optim.Adam(peephole_lstm_batch.parameters(), lr = 1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma=0.9)\n",
    "\n",
    "# epochs and batchsize\n",
    "n_epochs=20\n",
    "batch_size=10\n",
    "\n",
    "# training\n",
    "all_train_epoch_losses = []\n",
    "all_valid_epoch_losses = []\n",
    "all_epoch_perplexity = []\n",
    "all_epoch_accuracies = []\n",
    "\n",
    "# training\n",
    "for epoch in range(n_epochs):\n",
    "    random.shuffle(train_tensor)\n",
    "    train_epoch_loss = 0\n",
    "    valid_epoch_loss = 0\n",
    "    peephole_lstm_batch.train()\n",
    "\n",
    "    for batch in range(0, len(train_tensor), batch_size):\n",
    "        batch_data = train_tensor[batch:batch+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "  \n",
    "        sentence_length = [len(sentence) for sentence in batch_data]\n",
    "        max_len = max(sentence_length)\n",
    "        \n",
    "        batched_input = torch.empty(len(batch_data), max_len-1, dtype=torch.int64).to(device)\n",
    "        batched_mask = torch.zeros(len(batch_data), max_len-1, dtype=torch.bool).to(device)\n",
    "        batch_labels = torch.zeros(len(batch_data), max_len-1, dtype=torch.int64).to(device)\n",
    "        \n",
    "        batched_input.fill_(peephole_lstm_batch.embeddings.padding_idx)\n",
    "        \n",
    "        for i in range(len(batch_data)):\n",
    "            sentence = batch_data[i]\n",
    "            batched_input[i][:len(sentence) - 1] = sentence[:-1]\n",
    "            batched_mask[i][:len(sentence) - 1] = True\n",
    "            batch_labels[i][:len(sentence) - 1] = sentence[1:]\n",
    "\n",
    "        batch_logits = peephole_lstm_batch(batched_input)\n",
    "        batch_logits = batch_logits[batched_mask]\n",
    "        batch_labels = batch_labels[batched_mask]\n",
    "        \n",
    "        batch_loss = F.cross_entropy(batch_logits, batch_labels, reduction=\"sum\")\n",
    "        batch_norm = sum(len(s) - 1 for s in batch_data)\n",
    "            \n",
    "        train_epoch_loss += batch_loss.item()\n",
    "        loss = batch_loss / batch_norm\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # evaluation   \n",
    "    peephole_lstm_batch.eval()\n",
    "    n_valid_perplexity = []\n",
    "    accuracy = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in range(0, len(valid_tensor), batch_size):\n",
    "            batch_data = valid_tensor[batch:batch+batch_size]\n",
    "  \n",
    "            sentence_length = [len(sentence) for sentence in batch_data]\n",
    "            max_len = max(sentence_length)\n",
    "        \n",
    "            batched_input = torch.empty(len(batch_data), max_len-1, dtype=torch.int64).to(device)\n",
    "            batched_mask = torch.zeros(len(batch_data), max_len-1, dtype=torch.bool).to(device)\n",
    "            batch_labels = torch.zeros(len(batch_data), max_len-1, dtype=torch.int64).to(device)\n",
    "            \n",
    "            batched_input.fill_(peephole_lstm_batch.embeddings.padding_idx)\n",
    "            \n",
    "            for i in range(len(batch_data)):\n",
    "                sentence = batch_data[i]\n",
    "                batched_input[i][:len(sentence) - 1] = sentence[:-1]\n",
    "                batched_mask[i][:len(sentence) - 1] = True\n",
    "                batch_labels[i][:len(sentence) - 1] = sentence[1:]\n",
    "\n",
    "            batch_logits = peephole_lstm_batch(batched_input)\n",
    "            batch_logits = batch_logits[batched_mask]\n",
    "            batch_labels = batch_labels[batched_mask]\n",
    "            \n",
    "            batch_loss = F.cross_entropy(batch_logits, batch_labels, reduction=\"sum\")\n",
    "            batch_norm = sum(len(s) - 1 for s in batch_data)\n",
    "            valid_epoch_loss += batch_loss.item()\n",
    "\n",
    "            # calculate the perplexity\n",
    "            n_valid_perplexity.append(torch.exp(F.cross_entropy(batch_logits, batch_labels, reduction=\"mean\")).item())\n",
    "\n",
    "            # calculate the accuracy\n",
    "            accuracy.append((batch_labels == batch_logits.argmax(dim=1)).sum().item() / len(batch_labels))\n",
    "    \n",
    "    all_train_epoch_losses.append(train_epoch_loss / len(train_tensor))\n",
    "    all_valid_epoch_losses.append(valid_epoch_loss / len(valid_tensor))\n",
    "    all_epoch_perplexity.append(np.sum(n_valid_perplexity)/(len(n_valid_perplexity)))\n",
    "    all_epoch_accuracies.append(np.sum(accuracy) / len(accuracy))\n",
    "\n",
    "    print(\n",
    "        epoch, \":\\t / loss for train data:\\t\",\n",
    "        train_epoch_loss  / len(train_tensor),\n",
    "        \"\\t / loss for validation data:\\t\",\n",
    "        valid_epoch_loss  / len(valid_tensor),\n",
    "        \"\\t / perplexity:\\t\",\n",
    "        np.sum(n_valid_perplexity) / (len(n_valid_perplexity)),\n",
    "        \"\\t / accuracy:\\t\",\n",
    "        np.sum(accuracy) / len(accuracy),\n",
    "        flush=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xtIBuucsW0Im",
   "metadata": {
    "id": "xtIBuucsW0Im"
   },
   "outputs": [],
   "source": [
    "# save data\n",
    "p_perplexity = torch.tensor(all_epoch_perplexity)\n",
    "p_accuracies = torch.tensor(all_epoch_accuracies)\n",
    "p_train_losses = torch.tensor(all_train_epoch_losses)\n",
    "p_valid_losses = torch.tensor(all_valid_epoch_losses)\n",
    "\n",
    "torch.save(p_perplexity, \"./p_perplexity.pt\")\n",
    "torch.save(p_accuracies, \"./p_accuracies.pt\")\n",
    "torch.save(p_train_losses, \"./p_train_losses.pt\")\n",
    "torch.save(p_valid_losses, \"./p_valid_losses.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toHNfnA7aScB",
   "metadata": {
    "id": "toHNfnA7aScB"
   },
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KT6tpLDJaQrY",
   "metadata": {
    "id": "KT6tpLDJaQrY"
   },
   "outputs": [],
   "source": [
    "x1 = range(0, n_epochs)\n",
    "x2 = range(0, n_epochs)\n",
    "x3 = range(0, n_epochs)\n",
    "x4 = range(0, n_epochs)\n",
    "\n",
    "y1 = all_epoch_perplexity\n",
    "y2 = all_train_epoch_losses\n",
    "y3 = all_valid_epoch_losses\n",
    "y4 = all_epoch_accuracies\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(x1, y1)\n",
    "plt.title('Valid perplexity vs. epoches')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Valid perplexity')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(x4, y4)\n",
    "plt.title('Accuracy vs. epoches')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x2, y2,  label='Train loss')\n",
    "plt.plot(x3, y3, label='Valid loss')\n",
    "plt.title('Train/Valid loss vs. epoches')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240da8f",
   "metadata": {
    "id": "9240da8f"
   },
   "source": [
    "# ON-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a6b52e",
   "metadata": {
    "id": "70a6b52e"
   },
   "source": [
    "## ON-LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efe4f31",
   "metadata": {
    "id": "4efe4f31"
   },
   "outputs": [],
   "source": [
    "class ON_LSTM_cell(nn.Module):\n",
    "    \"\"\"ON LSTM Cell\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(ON_LSTM_cell, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # input gate\n",
    "        self.W_i = Parameter(Tensor(hidden_size, input_size))\n",
    "        self.U_i = Parameter(Tensor(hidden_size, hidden_size))\n",
    "        self.b_i = Parameter(Tensor(hidden_size, 1))\n",
    "        \n",
    "        # forget gate\n",
    "        self.W_f = Parameter(Tensor(hidden_size, input_size))\n",
    "        self.U_f = Parameter(Tensor(hidden_size, hidden_size))\n",
    "        self.b_f = Parameter(Tensor(hidden_size, 1))\n",
    "        \n",
    "        # output gate\n",
    "        self.W_o = Parameter(Tensor(hidden_size, input_size))\n",
    "        self.U_o = Parameter(Tensor(hidden_size, hidden_size))\n",
    "        self.b_o = Parameter(Tensor(hidden_size, 1))\n",
    "        \n",
    "        # cell\n",
    "        self.W_g = Parameter(Tensor(hidden_size, input_size))\n",
    "        self.U_g = Parameter(Tensor(hidden_size, hidden_size))\n",
    "        self.b_g = Parameter(Tensor(hidden_size, 1))\n",
    "        \n",
    "        # master input gate\n",
    "        self.W_mi = Parameter(Tensor(hidden_size, input_size))\n",
    "        self.U_mi = Parameter(Tensor(hidden_size, hidden_size))\n",
    "        self.b_mi = Parameter(Tensor(hidden_size, 1))\n",
    "        \n",
    "        # master forget gate\n",
    "        self.W_mf = Parameter(Tensor(hidden_size, input_size))\n",
    "        self.U_mf = Parameter(Tensor(hidden_size, hidden_size))\n",
    "        self.b_mf = Parameter(Tensor(hidden_size, 1))\n",
    "        \n",
    "        # initialize the weights\n",
    "        self.initialize_weights()\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        \"\"\"initialize weights\n",
    "        \"\"\"\n",
    "        '''\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            init.uniform_(weight, -stdv, stdv)\n",
    "        '''\n",
    "        for weight in self.parameters():\n",
    "            init.xavier_normal_(weight)\n",
    "    \n",
    "    def cumsoftmax(self, x):\n",
    "\n",
    "        return torch.cumsum(F.softmax(x, dim=-1), dim=-1)\n",
    "    \n",
    "    def forward(self, inputs, state = None):\n",
    "        \"\"\"Forward\n",
    "        Args:\n",
    "            inputs: [1, 1, input_size]\n",
    "            state: ([1, 1, hidden_size], [1, 1, hidden_size])\n",
    "        \"\"\"\n",
    "        batch_size, seq_size, _ = inputs.size()\n",
    "        hidden_seq = []\n",
    "        \n",
    "        # read the state\n",
    "        if state is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size).t().to(inputs.device)\n",
    "            c_t = torch.zeros(batch_size, self.hidden_size).t().to(inputs.device)\n",
    "        else:\n",
    "            (h, c) = state\n",
    "            h_t = h.squeeze(0).t()\n",
    "            c_t = c.squeeze(0).t()\n",
    "        \n",
    "        # for each sequence, do the iteration\n",
    "        for t in range(seq_size):\n",
    "            x_t = inputs[:, t, :].t()\n",
    "            \n",
    "            # input gate\n",
    "            i_t = torch.sigmoid(self.W_i @ x_t + self.U_i @ h_t + self.b_i)\n",
    "            # forget gate\n",
    "            f_t = torch.sigmoid(self.W_f @ x_t + self.U_f @ h_t + self.b_f)\n",
    "            # cell\n",
    "            g_t = torch.tanh(self.W_g @ x_t + self.U_g @ h_t + self.b_g)\n",
    "            # output gate\n",
    "            o_t = torch.sigmoid(self.W_o @ x_t + self.U_o @ h_t + self.b_o)\n",
    "            \n",
    "            # master input gate\n",
    "            im_t = 1. - self.cumsoftmax(self.W_mi @ x_t + self.U_mi @ h_t + self.b_mi)\n",
    "            # master forget gate\n",
    "            fm_t = self.cumsoftmax(self.W_mf @ x_t + self.U_mf @ h_t + self.b_mf)\n",
    "            \n",
    "            w_t = f_t * i_t\n",
    "            c_t = w_t * (f_t * c_t + i_t * g_t) + (fm_t-w_t) * c_t + (im_t-w_t) * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            \n",
    "           \n",
    "            hidden_seq.append(h_t.t().unsqueeze(0))\n",
    "        \n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq =hidden_seq.transpose(0,1).contiguous()\n",
    "        \n",
    "        return hidden_seq, (h_t.t().unsqueeze(0), c_t.t().unsqueeze(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcf1ebe",
   "metadata": {
    "id": "bfcf1ebe"
   },
   "source": [
    "## ON-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118636b3",
   "metadata": {
    "id": "118636b3"
   },
   "outputs": [],
   "source": [
    "# without batch\n",
    "class ON_LSTM(nn.Module):\n",
    "    def __init__(self, total_num_words, emb_size=100, lstm_hidden_size=200, n_lstm_layers=1):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(total_num_words, emb_size)\n",
    "        self.lstm = ON_LSTM_cell(input_size = emb_size, hidden_size = lstm_hidden_size)\n",
    "        self.output_proj = nn.Linear(lstm_hidden_size, total_num_words)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        embedding = self.embeddings(input_tensor)\n",
    "        hidden, (h, c) = self.lstm(embedding.unsqueeze(0))\n",
    "        output_logics = self.output_proj(hidden)\n",
    "        \n",
    "        return output_logics.squeeze(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8718774e",
   "metadata": {
    "id": "8718774e"
   },
   "outputs": [],
   "source": [
    "# with batch\n",
    "class ON_LSTM_batch(nn.Module):\n",
    "    def __init__(self, total_num_words, emb_size=400, lstm_hidden_size=1000, n_lstm_layers=1):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(total_num_words+1, emb_size, padding_idx = len(train_words))\n",
    "        self.lstm = ON_LSTM_cell(input_size = emb_size, hidden_size = lstm_hidden_size)\n",
    "        self.output_proj = nn.Linear(lstm_hidden_size, total_num_words)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        embedding = self.embeddings(input_tensor)\n",
    "        hidden, (h, c) = self.lstm(embedding)\n",
    "        output_logics = self.output_proj(hidden)\n",
    "        \n",
    "        return output_logics\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdccae54",
   "metadata": {
    "id": "cdccae54"
   },
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c835c60c",
   "metadata": {
    "id": "c835c60c"
   },
   "source": [
    "### without batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402b309",
   "metadata": {
    "id": "e402b309"
   },
   "outputs": [],
   "source": [
    "# on lstm model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "on_lstm = ON_LSTM(len(train_words)).to(device)\n",
    "\n",
    "# optimizer \n",
    "optimizer = torch.optim.Adam(on_lstm.parameters(), lr = 1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma=0.9)\n",
    "\n",
    "# training\n",
    "all_epoch_train_losses = []\n",
    "all_epoch_valid_losses = []\n",
    "all_epoch_perplexity = []\n",
    "\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    # learning\n",
    "    random.shuffle(train_tensor)\n",
    "    train_epoch_loss = 0\n",
    "    valid_epoch_loss = 0\n",
    "    on_lstm.train()\n",
    "    for sentence in train_tensor:\n",
    "        optimizer.zero_grad() ## zero the parameters of gradients\n",
    "        logits = on_lstm(sentence[:-1].to(device)) \n",
    "\n",
    "        sentence_loss = F.cross_entropy(logits, sentence[1:].to(device), reduction=\"sum\")\n",
    "        \n",
    "        train_epoch_loss += sentence_loss.item()\n",
    "        loss = sentence_loss / (len(sentence)-1)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "        \n",
    "    # evaluation   \n",
    "    on_lstm.eval()\n",
    "    n_valid_perplexity = []\n",
    "                     \n",
    "    with torch.no_grad():\n",
    "        for sentence in valid_tensor:\n",
    "            logits = on_lstm(sentence[:-1].to(device))\n",
    "            sentence_loss = F.cross_entropy(logits, sentence[1:].to(device), reduction=\"sum\")\n",
    "            valid_epoch_loss += sentence_loss.item()\n",
    "              \n",
    "            n_valid_perplexity.append(torch.exp(F.cross_entropy(logits, sentence[1:].to(device),reduction='mean')).item())\n",
    "            \n",
    "    all_train_epoch_losses.append(train_epoch_loss / len(train_tensor))\n",
    "    all_valid_epoch_losses.append(valid_epoch_loss / len(valid_tensor))\n",
    "    all_epoch_perplexity.append(np.sum(n_valid_perplexity) / len(n_valid_perplexity)) \n",
    "    \n",
    "    print(\n",
    "        epoch, \":\\t / loss for train data:\\t\",\n",
    "        train_epoch_loss / len(train_tensor),\n",
    "        \"\\t / loss for validation data:\\t\",\n",
    "        valid_epoch_loss / len(valid_tensor),\n",
    "        \"\\t / perplexity:\\t\",\n",
    "        np.sum(n_valid_perplexity) / len(n_valid_perplexity),\n",
    "        flush=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0accc624",
   "metadata": {
    "id": "0accc624"
   },
   "source": [
    "### with batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef71339",
   "metadata": {
    "id": "aef71339"
   },
   "outputs": [],
   "source": [
    "# on lstm model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "on_lstm_batch = ON_LSTM_batch(len(train_words)).to(device)\n",
    "\n",
    "# optimizer \n",
    "optimizer = torch.optim.Adam(on_lstm_batch.parameters(), lr = 1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma=0.9)\n",
    "\n",
    "# epochs and batchsize\n",
    "n_epochs=20\n",
    "batch_size=10\n",
    "\n",
    "# training\n",
    "all_train_epoch_losses = []\n",
    "all_valid_epoch_losses = []\n",
    "all_epoch_perplexity = []\n",
    "all_epoch_accuracies = []\n",
    "\n",
    "# training\n",
    "for epoch in range(n_epochs):\n",
    "    random.shuffle(train_tensor)\n",
    "    train_epoch_loss = 0\n",
    "    valid_epoch_loss = 0\n",
    "    on_lstm_batch.train()\n",
    "\n",
    "    for batch in range(0, len(train_tensor), batch_size):\n",
    "        batch_data = train_tensor[batch:batch+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "  \n",
    "        sentence_length = [len(sentence) for sentence in batch_data]\n",
    "        max_len = max(sentence_length)\n",
    "        \n",
    "        batched_input = torch.empty(len(batch_data), max_len-1, dtype=torch.int64).to(device)\n",
    "        batched_mask = torch.zeros(len(batch_data), max_len-1, dtype=torch.bool).to(device)\n",
    "        batch_labels = torch.zeros(len(batch_data), max_len-1, dtype=torch.int64).to(device)\n",
    "        \n",
    "        batched_input.fill_(on_lstm_batch.embeddings.padding_idx)\n",
    "        \n",
    "        for i in range(len(batch_data)):\n",
    "            sentence = batch_data[i]\n",
    "            batched_input[i][:len(sentence) - 1] = sentence[:-1]\n",
    "            batched_mask[i][:len(sentence) - 1] = True\n",
    "            batch_labels[i][:len(sentence) - 1] = sentence[1:]\n",
    "\n",
    "        batch_logits = on_lstm_batch(batched_input)\n",
    "        batch_logits = batch_logits[batched_mask]\n",
    "        batch_labels = batch_labels[batched_mask]\n",
    "        \n",
    "        batch_loss = F.cross_entropy(batch_logits, batch_labels, reduction=\"sum\")\n",
    "        batch_norm = sum(len(s) - 1 for s in batch_data)\n",
    "            \n",
    "        train_epoch_loss += batch_loss.item()\n",
    "        loss = batch_loss / batch_norm\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # evaluation   \n",
    "    on_lstm_batch.eval()\n",
    "    n_valid_perplexity = []\n",
    "    accuracy = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in range(0, len(valid_tensor), batch_size):\n",
    "            batch_data = valid_tensor[batch:batch+batch_size]\n",
    "  \n",
    "            sentence_length = [len(sentence) for sentence in batch_data]\n",
    "            max_len = max(sentence_length)\n",
    "        \n",
    "            batched_input = torch.empty(len(batch_data), max_len-1, dtype=torch.int64).to(device)\n",
    "            batched_mask = torch.zeros(len(batch_data), max_len-1, dtype=torch.bool).to(device)\n",
    "            batch_labels = torch.zeros(len(batch_data), max_len-1, dtype=torch.int64).to(device)\n",
    "            \n",
    "            batched_input.fill_(on_lstm_batch.embeddings.padding_idx)\n",
    "            \n",
    "            for i in range(len(batch_data)):\n",
    "                sentence = batch_data[i]\n",
    "                batched_input[i][:len(sentence) - 1] = sentence[:-1]\n",
    "                batched_mask[i][:len(sentence) - 1] = True\n",
    "                batch_labels[i][:len(sentence) - 1] = sentence[1:]\n",
    "\n",
    "            batch_logits = on_lstm_batch(batched_input)\n",
    "            batch_logits = batch_logits[batched_mask]\n",
    "            batch_labels = batch_labels[batched_mask]\n",
    "            \n",
    "            batch_loss = F.cross_entropy(batch_logits, batch_labels, reduction=\"sum\")\n",
    "            batch_norm = sum(len(s) - 1 for s in batch_data)\n",
    "            valid_epoch_loss += batch_loss.item()\n",
    "\n",
    "            # calculate the perplexity\n",
    "            n_valid_perplexity.append(torch.exp(F.cross_entropy(batch_logits, batch_labels, reduction=\"mean\")).item())\n",
    "\n",
    "            # calculate the accuracy\n",
    "            accuracy.append((batch_labels == batch_logits.argmax(dim=1)).sum().item() / len(batch_labels))\n",
    "            \n",
    "    all_train_epoch_losses.append(train_epoch_loss / len(train_tensor))\n",
    "    all_valid_epoch_losses.append(valid_epoch_loss / len(valid_tensor))\n",
    "    all_epoch_perplexity.append(np.sum(n_valid_perplexity)/(len(n_valid_perplexity) * batch_size))\n",
    "    all_epoch_accuracies.append(np.sum(accuracy) / len(accuracy))\n",
    "\n",
    "    print(\n",
    "        epoch, \":\\t / loss for train data:\\t\",\n",
    "        train_epoch_loss / len(train_tensor),\n",
    "        \"\\t / loss for validation data:\\t\",\n",
    "        valid_epoch_loss / len(valid_tensor),\n",
    "        \"\\t / perplexity:\\t\",\n",
    "        np.sum(n_valid_perplexity) / (len(n_valid_perplexity)),\n",
    "        \"\\t / accuracy:\\t\",\n",
    "        np.sum(accuracy) / len(accuracy),\n",
    "        flush=True\n",
    "    )\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CLmebvANWrTu",
   "metadata": {
    "id": "CLmebvANWrTu"
   },
   "outputs": [],
   "source": [
    "# save data\n",
    "on_perplexity = torch.tensor(all_epoch_perplexity)\n",
    "on_accuracies = torch.tensor(all_epoch_accuracies)\n",
    "on_train_losses = torch.tensor(all_train_epoch_losses)\n",
    "on_valid_losses = torch.tensor(all_valid_epoch_losses)\n",
    "\n",
    "torch.save(on_perplexity, \"./on_perplexity.pt\")\n",
    "torch.save(on_accuracies, \"./on_accuracies.pt\")\n",
    "torch.save(on_train_losses, \"./on_train_losses.pt\")\n",
    "torch.save(on_valid_losses, \"./on_valid_losses.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fc9mWgM4anVe",
   "metadata": {
    "id": "Fc9mWgM4anVe"
   },
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a303f6",
   "metadata": {
    "id": "36a303f6"
   },
   "outputs": [],
   "source": [
    "\n",
    "x1 = range(0, n_epochs)\n",
    "x2 = range(0, n_epochs)\n",
    "x3 = range(0, n_epochs)\n",
    "x4 = range(0, n_epochs)\n",
    "\n",
    "y1 = all_epoch_perplexity\n",
    "y2 = all_train_epoch_losses\n",
    "y3 = all_valid_epoch_losses\n",
    "y4 = all_epoch_accuracies\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(x1, y1)\n",
    "plt.title('Valid perplexity vs. epoches')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Valid perplexity')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(x4, y4)\n",
    "plt.title('Accuracy vs. epoches')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x2, y2,  label='Train loss')\n",
    "plt.plot(x3, y3, label='Valid loss')\n",
    "plt.title('Train/Valid loss vs. epoches')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wmYEFUsAU29A",
   "metadata": {
    "id": "wmYEFUsAU29A"
   },
   "outputs": [],
   "source": [
    "# Plot accuracy in one figure\n",
    "x1 = range(0, n_epochs)\n",
    "x2 = range(0, n_epochs)\n",
    "x3 = range(0, n_epochs)\n",
    "\n",
    "on_acc = torch.load(\"./on_accuracies.pt\")\n",
    "p_acc = torch.load(\"./p_accuracies.pt\")\n",
    "naive_acc = torch.load(\"./naive_accuracies.pt\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x1, naive_acc,  label='naive LSTM')\n",
    "\n",
    "plt.plot(x2, p_acc,  label='peephole LSTM')\n",
    "\n",
    "plt.plot(x3, on_acc, label='on LSTM')\n",
    "plt.title('Accuracy vs. epoches')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gspAKmUYU5t1",
   "metadata": {
    "id": "gspAKmUYU5t1"
   },
   "outputs": [],
   "source": [
    "# Plot perplexity in one figure\n",
    "x1 = range(0, n_epochs)\n",
    "x2 = range(0, n_epochs)\n",
    "x3 = range(0, n_epochs)\n",
    "\n",
    "on_p = torch.load(\"./on_perplexity.pt\")\n",
    "p_p = torch.load(\"./p_perplexity.pt\")\n",
    "naive_p = torch.load(\"./naive_perplexity.pt\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x1, naive_p,  label='naive LSTM')\n",
    "\n",
    "plt.plot(x2, p_p,  label='peephole LSTM')\n",
    "\n",
    "plt.plot(x3, on_p, label='on LSTM')\n",
    "plt.title('Perplexity vs. epoches')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('perplexity')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22A7RXFMWd-t",
   "metadata": {
    "id": "22A7RXFMWd-t"
   },
   "outputs": [],
   "source": [
    "# Plot train and validation losses for 3 LSTM models\n",
    "x1 = range(0, n_epochs)\n",
    "x2 = range(0, n_epochs)\n",
    "x3 = range(0, n_epochs)\n",
    "x4 = range(0, n_epochs)\n",
    "x5 = range(0, n_epochs)\n",
    "x6 = range(0, n_epochs)\n",
    "\n",
    "\n",
    "on_v_loss = torch.load(\"./on_valid_losses.pt\")\n",
    "p_v_loss = torch.load(\"./p_valid_losses.pt\")\n",
    "naive_v_loss = torch.load(\"./naive_valid_losses.pt\")\n",
    "\n",
    "\n",
    "on_t_loss = torch.load(\"./on_train_losses.pt\")\n",
    "p_t_loss = torch.load(\"./p_train_losses.pt\")\n",
    "naive_t_loss = torch.load(\"./naive_train_losses.pt\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x1, naive_v_loss,  label='Validation loss')\n",
    "plt.plot(x2, naive_t_loss,  label='Train loss')\n",
    "\n",
    "plt.title('Losses from naive LSTM vs. epoches')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x3, p_v_loss,  label='Validation loss')\n",
    "plt.plot(x4, p_t_loss,  label='Train loss')\n",
    "\n",
    "plt.title('Losses from Peephole LSTM vs. epoches')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x5, on_v_loss,  label='Validation loss')\n",
    "plt.plot(x6, on_t_loss,  label='Train loss')\n",
    "\n",
    "plt.title('Losses from On LSTM vs. epoches')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "215px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
